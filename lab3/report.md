# Computer Networking Lab -- Socket Programming Report

纪信佑 519021910838

[toc]

## Directories

├── **P2P**
│   ├── **P2P.py**
│   ├── **client.py**
│   ├── **file.txt**
│   └── **server.py**
├── \__pycache__
│   └── macro.cpython-38.pyc
├── batchCS
│   ├── run10.cli
│   ├── run12.cli
│   ├── run2.cli
│   ├── run4.cli
│   ├── run6.cli
│   ├── run8.cli
│   └── startsvr.cli
├── batchP2P
│   ├── run10.cli
│   ├── run12.cli
│   ├── run2.cli
│   ├── run4.cli
│   ├── run6.cli
│   └── run8.cli
├── **clientServer**
│   ├── **CS.py**
│   ├── **client.py**
│   ├── **file.txt**
│   └── **server.py**
├── **macro.json**
└── **report.md**

## What are the files?

* **P2P**: In P2P directory, files are...
  * P2P.py: the topology of P2P experiment
  * client.py & server.py: the python file which clients and server run respectively
  * file.txt: generated by the server.py
    * you can manually assign literally everything in **macro.json**, including the size of file, the # of host, etc.
* **Client/Server**: In  clientServer directory, files are...
  * CS.py: the topology
  * client.py & server.py: the python file which clients and server run respectively
  * file.txt: generated by the server.py
* **batchCS & batchP2P**:
  * under these two directories, several shell scirpts are written for convenience
  * run#.cli means it is the shell script to run the test with # of **clients** 
    * e.g. run8.cli is the shell script to run the test with 8 clients and 1 server
    * **NOTE: ALL THE FILES ARE GOING TO BE DELETED BY THE SHELL SCRIPT AFTER THE TEST**
      * **if you want to keep the file, delete the last line of the shell script**
        *  i.e. the line with h1 rm -rf save*

* **macro.json**
  * In this file, all the macros are defined, including **the size of file, the maximum number of files created, the number of clients, the number of threads run by the server, ....etc.**

## How to run the files?

If any unreasonable bug happens in your second test (only sometimes),  you may restart the topology to test again so as to get the correct result. **(Normally it should be fine to test many time within one boot of topology)**

**You may see some info outputs to stdout which do not belong to previous test**, this is because the hosts run in the background and the outputs are stored in the buffer, when you run another test, the buffer will be cleared and output to stdout, Don't worry.

Also, if you change the filesize, after you start the server, please wait for some seconds to start the clients because it takes some seconds for server to create a large file. (Normally it won't take more than 1 sec)

### ClientServer Model files

* `cd clientServer` to get into the directory
* `sudo mn -c` to clean up the mess  (**optional**, do this only when exception happens)
* `sudo python3 CS.py` to **start the topology**
  * before you run the batchfile, **make sure you have set enough NR_HOST macro in macro.json** to run the destined batchfile
* `h0 python3 server.py &` to start the server

* `source ../batchCS/run#.cli` to run the batchfile, the server will respond the clients' requests and send the file to them in a multithread manner



### P2P Model files

* `cd P2P` to get in to the directory

* `sudo mn -c` to clean up the mess (**optional**, do this only when exception happens)

* `sudo python3 P2P.py` to start the topology, to be fair, the CS model and P2P model uses the same topology

  * **IMPORTANT**: the **NR_HOST ** macro needs to be set **correctly**, coresponding to your batchfile name, **before you start the topology and server**! **If** **it is not set correctly, it WILL NOT WORK!**

    Also, remeber that the FILESIZE macro should be **set before you start the server!**

* `h0 python3 server.py &` to start the server

* `source ../batchP2P/run#.cli` to run the test

Due to the implementation, one time of topology boot can only do one test. If you want to test for multiple times, restart your topology :(

## The implementation

### Client/Server Model

#### Topology

The topology tested looks like below, the server is a running host in mininet.

<img src="/Users/mac/Downloads/blank-34.jpg" alt="blank-34" style="zoom:25%;" />

#### logic

##### Server

the server works in a multithread manner, serving several clients simultaneously.

the server first create a file to transfer through bash commands

* cat /dev/null > file.txt
* echo "HELLOHELLOHELLO\nWORLDWORLDWORLD\n" > file.txt (32bytes)
* cat file.txt file.txt > file2.txt && mv file2.txt file.txt (do this several times)

the server threads work like this:

* It initialize a socket to listen for request at a destined port at local
* Once it receive a request, it accepts it and initialize another socket to transfer the file to the client, through that socket. 
* After the transmission, the new socket is closed.

##### Client

the client's logic is very simple in CS model

It knows server's ip address and port in advance, create a socket to connect to it (i.e. send a request)

Then it receive the byte stream coming from the server in order and save it to local directory

### P2P Model

The topology is the same as CS Model, which is shown above already, to be fair to test the performance between them

#### logic

##### Server

Unlike the CS model, whose server is more complex, P2P model server has a simpler logic comparing to its client. However it is still more complex than server logic in CS model.

In the implementation, the chunk is not distributed completely free, the first chunk of file is always in h1, the second chunk of file is always in h2, ... etc.

In the server, it has a table of chunk size (easily calculate without outer information), table of clients , the entire file, and **most importantly the table of clients and chunks corresponding**

It looks like this **logically**: (in code it is not this integrated)

| 10.0.0.2 | Chunk1 | 1024B |
| -------- | ------ | ----- |
| 10.0.0.3 | Chunk2 | 1024B |
| 10.0.0.4 | Chunk3 | 1024B |
| 10.0.0.5 | Chunk4 | 1024B |

The server cut the file in chunks and send them to clients (which has already issued an request) accordingly.

##### Client

The logic of Client in P2P is complex, because in this architecture it serves not only a bare client, it is also a running server in other clients' point of view.

In the implementation, the client initialize two threads, named **Listening_Thread** and **Download_Thread**

**Note that the table shown above is shared only logically by clients and server because the chunks are not completely free**

**Listen_Thread**:

The ListenThread is the serving thread of the client, it has mainly two jobs:

* Download the local chunk from server to local
* Transfer the local file chunk to other clients which is requesting it

The two jobs can only be done in a serial manner since only when the local chunk is ready can the thread serves them to other clients

**Download_Thread**:

The download thread only has two jobs as well:

* Request all remote chunks at other clients and download them
* when all the chunks are ready, save them in order to local directory

The two jobs can only be done serially for the similar reason.

In order the save the file in order, the chunks are downloaded to a local buffer before, then save them in order by sequentially indexing them.

These two threads run simutaneously each client, and the chunk is transferred in a distributed way, utilizing the resource better and hopefully has a better result.

## Comparison & Analysis

**The test is done with Server thread # = 4**

### Client/Server Model

| # of clients \ Filesize | 4MB    | 8MB    | 16MB    |
| :---------------------- | ------ | ------ | ------- |
| 2                       | 11.66s | 19.10s | 35.43s  |
| 4                       | 16.39s | 35.54s | 59.85s  |
| 6                       | 25.60s | 48.70s | 89.75s  |
| 8                       | 33.55s | 61.84s | 109.31s |
| 10                      | 44.45s | 79.01s | 143.26s |
| 12                      | 47.01s | 90.81s | 170.71s |

### P2P Model

| # of clients \ Filesize | 8MB    | 16MB   |
| :---------------------- | ------ | ------ |
| 2                       | 10.08s | 41.98s |
| 4                       | 6.70s  | 27.29s |
| 6                       | 6.70s  | 22.99s |
| 8                       | 6.41s  | 23.12s |
| 10                      | 7.69s  | 22.33s |
| 12                      | 10.63s | 22.93s |

### Analysis

From the tables above, we can see that, in nearly all cases (except the one with big file and less clients in P2P group, which makes sense), P2P's performance crushes the traditional Client/Server Model. 

However, this makes sense because the clients' resources are often not utilized best in Client/Server Model due to the bandwidth limit of server.

**In Client Server Model**, the time expired transfering files increments almost linearly along with the file size increment, which fits our analysis in class.

<img src="/Users/mac/Documents/gitrepos/SJTU-networking-labs/lab3/cs.png" alt="cs" style="zoom: 67%;" />

**In P2P Model**, the time expired gradually decrements along with more clients joining the P2P group, this makes sense as well because more clients in group allows the clients to utilize their resources better.

However, when the number of clients reaches a certain level, the time expired stops decrementing. This means the resources are almost fully utitilized.

<img src="/Users/mac/Documents/gitrepos/SJTU-networking-labs/lab3/p2p.png" alt="p2p" style="zoom: 67%;" />

**In comparison**, the result is shown below, P2P is way better!

<img src="/Users/mac/Documents/gitrepos/SJTU-networking-labs/lab3/comp.png" alt="comp" style="zoom: 67%;" />